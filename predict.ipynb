{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "responsible-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tcupgan import DataGenerator, LSTMUNet\n",
    "from tcupgan.disc import PatchDiscriminator\n",
    "from tcupgan.losses import fc_tversky\n",
    "from tcupgan.vanilla import TemporalUNet\n",
    "from tcupgan.fc_utils import query_images_and_masks\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "from torchinfo import summary\n",
    "%matplotlib inline\n",
    "import glob, json\n",
    "import netCDF4 as nc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d69911",
   "metadata": {},
   "source": [
    "Initialize the LSTM and 3D Conv U-Net architectures so we can compare performance on both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dress-handling",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "hidden = [8, 18, 24, 32, 64, 128, 128]\n",
    "bottleneck_dims = [64, 32, 16, 8]\n",
    "generator_lstm = LSTMUNet(hidden_dims=hidden, bottleneck_dims=bottleneck_dims,\n",
    "                     input_channels=1, output_channels=1).to(device)\n",
    "# device = 'cuda'\n",
    "hidden = [8, 18, 24, 32, 64, 128, 128]\n",
    "bottleneck_dims = [64, 32, 16, 8]\n",
    "generator_unet = TemporalUNet(hidden_dims=hidden, bottleneck_dims=bottleneck_dims,\n",
    "                     input_channels=1, output_channels=1).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa104a9",
   "metadata": {},
   "source": [
    "Load the checkpoints for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bizarre-disclosure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator_unet.eval()\n",
    "generator_lstm.eval()\n",
    "\n",
    "generator_lstm.load_state_dict(torch.load('./checkpoints-FC.resized-gamma05/generator_ep_150.pth', map_location=device))\n",
    "generator_unet.load_state_dict(torch.load('./checkpoints-3DUNET/generator_ep_150.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "elect-intellectual",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred(img, which_gen):\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    with torch.no_grad():\n",
    "        IMG = torch.Tensor(img).to(device)\n",
    "        pred = np.array(which_gen(IMG).cpu().numpy()[0])\n",
    "        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2484930",
   "metadata": {},
   "source": [
    "Load a list of image slices and corresponding subject IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "boolean-communications",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './image_stack_list.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./image_stack_list.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m     slice_mapping \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './image_stack_list.json'"
     ]
    }
   ],
   "source": [
    "with open('./image_stack_list.json', 'r') as f:\n",
    "    slice_mapping = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "verified-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 50494631 has 50 slices\n",
      "Subject 56399759 has 25 slices\n",
      "Subject 56399824 has 60 slices\n",
      "Subject 56399954 has 189 slices\n",
      "Subject 56400162 has 114 slices\n",
      "Subject 56400344 has 21 slices\n",
      "Subject 56400369 has 30 slices\n",
      "Subject 56400408 has 150 slices\n",
      "Subject 56400598 has 67 slices\n",
      "Subject 56400789 has 105 slices\n",
      "Subject 56477542 has 285 slices\n",
      "Subject 56478334 has 274 slices\n",
      "Subject 56479153 has 395 slices\n",
      "Subject 56480550 has 85 slices\n",
      "Subject 56480760 has 160 slices\n",
      "Subject 56481693 has 13 slices\n",
      "Subject 56481883 has 305 slices\n"
     ]
    }
   ],
   "source": [
    "all_slices = []\n",
    "for kk in slice_mapping:\n",
    "    if len(kk)>10:\n",
    "        print(f\"Subject {kk[0]} has {len(kk)} slices\")\n",
    "        all_slices.append(kk[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bf5e1f",
   "metadata": {},
   "source": [
    "Loop through each subject, build the image cube from the 2D slices and predict. We will save the outputs into a NetCDF file in the `3D_cube_renditions` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "express-blair",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 56479153\n"
     ]
    }
   ],
   "source": [
    "for each_sequence in all_slices:\n",
    "    matched_seq = [i for i in slice_mapping if each_sequence in i][0]\n",
    "    \n",
    "    # query the input images from the subject images and build the cube\n",
    "    # `umii_path` corresponds to the root folder containing the images and masks\n",
    "    # directories from the DataLad repo\n",
    "    imgs, masks = query_images_and_masks(matched_seq, umii_path='../umii-fatchecker-dataset/')\n",
    "    \n",
    "    data_3d = np.asarray(imgs[f'{matched_seq[0]}_4'])/255.\n",
    "    data_mask = np.asarray(masks[f'{matched_seq[0]}_4'])\n",
    "    \n",
    "    # get the prediction from the LSTM and 3D U-Nets\n",
    "    pred_mask_3d_unet = get_pred(data_3d, generator_unet)\n",
    "    pred_mask_3d_lstm = get_pred(data_3d, generator_lstm)\n",
    "\n",
    "    # save out to a NetCDF file\n",
    "    with nc.Dataset(f'./3D_cube_renditions/{matched_seq[0]}.nc', 'w') as dset:\n",
    "        dset.createDimension('x', 256)\n",
    "        dset.createDimension('y', 256)\n",
    "        dset.createDimension('z', len(data_3d))\n",
    "\n",
    "        imgVar = dset.createVariable('img', 'f8', ('x', 'y', 'z'))\n",
    "        unetVar = dset.createVariable('unet', 'f8', ('x', 'y', 'z'))\n",
    "        tcupVar = dset.createVariable('tcup', 'f8', ('x', 'y', 'z'))\n",
    "\n",
    "        imgVar[:] = np.transpose(np.squeeze(data_3d,axis=1), (2, 1, 0))\n",
    "        unetVar[:] = np.transpose(np.squeeze(pred_mask_3d_unet, axis=1), (2, 1, 0))\n",
    "        tcupVar[:] = np.transpose(np.squeeze(pred_mask_3d_lstm, axis=1), (2, 1, 0))\n",
    "\n",
    "    print(f'Done {each_sequence}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
